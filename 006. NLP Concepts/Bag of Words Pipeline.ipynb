{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Pipeline\n",
    "1. Get the Data/Corpus\n",
    "2. Tokenisation,Stopword Removal\n",
    "3. Stemming\n",
    "4. Building a Vocab\n",
    "5. Vectorization\n",
    "6. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Hello there! My name is Isha. Today is a very pleasant day and the weather is cool. :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting separate sentences\n",
    "t1=sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there!', 'My name is Isha.', 'Today is a very pleasant day and the weather is cool.', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting separate words/special characters\n",
    "t2=word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', '!', 'My', 'name', 'is', 'Isha', '.', 'Today', 'is', 'a', 'very', 'pleasant', 'day', 'and', 'the', 'weather', 'is', 'cool', '.', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It contains stopwords from all the languages but we only consider english stopwords\n",
    "eng_sw=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itself', 'a', \"won't\", 'which', \"aren't\", \"you're\", \"wasn't\", 'on', 'weren', 'to', 'do', 'with', \"she's\", 'isn', 'down', 'an', 'under', 'then', 'ma', 'here', 'most', 'we', 'had', 'ain', 'yourselves', 'and', 'as', 'being', \"shan't\", 'some', 'myself', 'for', 'm', 'if', 'didn', 'own', \"you'd\", 'very', 'yourself', \"hasn't\", 'can', 'in', 'herself', 'been', 'ours', 'aren', 'that', 'again', 'off', \"mightn't\", 'needn', 'it', \"you've\", 'of', 'once', \"should've\", 'you', 'have', 'my', 'themselves', 'should', \"didn't\", 'both', 'did', 'will', 'hasn', 'them', 'wasn', 'ourselves', 'into', 'she', 'through', 'there', 'is', 'by', \"mustn't\", 'mustn', 'until', 'having', 'too', 'above', 'only', \"weren't\", 'any', 'when', 'than', 'our', 'i', 'at', 'has', \"it's\", 'where', \"that'll\", 'before', 'all', 'each', 'were', 'theirs', \"doesn't\", 'why', 'y', 'against', 'those', 'few', 'whom', 'hadn', \"wouldn't\", 'about', \"hadn't\", 'your', 'nor', 'her', 'up', 'how', 'but', 'its', \"shouldn't\", 'or', \"couldn't\", \"isn't\", 'from', 'are', \"haven't\", \"needn't\", 've', 'yours', 'such', 'couldn', 'more', 'won', 'he', 'no', 'be', 'his', 'wouldn', 'this', 'himself', 'out', 'other', 're', 'hers', 'between', 's', 'haven', 'just', 'am', 'below', 'not', 'same', 'd', 'they', 'doing', 'was', \"you'll\", 'him', 'because', 'their', 'll', 'during', \"don't\", 'further', 'doesn', 'o', 'shan', 'so', 'these', 'after', 'me', 'shouldn', 't', 'who', 'the', 'what', 'don', 'mightn', 'does', 'now', 'over', 'while'}\n"
     ]
    }
   ],
   "source": [
    "print(eng_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(text,sw):\n",
    "    useful_words=[i for i in text if i not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=remove_sw('I am happy with my life'.split(),eng_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'happy', 'life']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=remove_sw('Hello there! My name is Isha. Today is a very pleasant day and the weather is cool.'.split(),eng_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there!', 'My', 'name', 'Isha.', 'Today', 'pleasant', 'day', 'weather', 'cool.']\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation using Regular Expression\n",
    "https://www.regexpal.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Hello there! Mic testing.. 1, 2, 3 Great its working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text=tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'there', 'Mic', 'testing..', 'Great', 'its', 'working']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Three types of Stemmer:\n",
    "    \n",
    "    1. Snowball\n",
    "    2.Porter\n",
    "    3.Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of a stemmer\n",
    "Ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps.stem('sitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-language Stemmer - specify lamguage\n",
    "Ss=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ss.stem('sitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wn=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wn.lemmatize('watching')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Vocab and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adventure,mystry,news,romance\n",
    "sample_corpus=[\n",
    "    \"It could be some kind of trick Budd had thought up.\",\n",
    "    \"I'm calling you , Mr. Nelson , at the request of Mr. Phillip Wycoff.\",\n",
    "    \"The senate quickly whipped through its meager fare of House bills approved by committees,passing the three on the calendar.\",\n",
    "    \"He expected Concetta's thin hand to reach down to grasp the boy,and her shrill,impetuous voice to sound against the rotundity of his disfigured flesh that was never sure of hearing anything.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer: \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus=Cv.fit_transform(sample_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 29)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 56)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 57)\t1\n",
      "  (1, 36)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 62)\t1\n",
      "  (1, 33)\t2\n",
      "  (1, 34)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 50)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 61)\t1\n",
      "  (2, 36)\t1\n",
      "  (2, 50)\t3\n",
      "  (2, 44)\t1\n",
      "  (2, 40)\t1\n",
      "  :\t:\n",
      "  (3, 13)\t1\n",
      "  (3, 51)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 55)\t3\n",
      "  (3, 41)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 20)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 25)\t1\n",
      "  (3, 45)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 58)\t1\n",
      "  (3, 47)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 43)\t1\n",
      "  (3, 26)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 19)\t1\n",
      "  (3, 49)\t1\n",
      "  (3, 59)\t1\n",
      "  (3, 35)\t1\n",
      "  (3, 48)\t1\n",
      "  (3, 24)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  It could be some kind of trick Budd had thought up.\n",
      "Vectorized form:  [0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#first line\n",
    "print(\"Original: \",sample_corpus[0])\n",
    "print(\"Vectorized form: \",vectorized_corpus.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 29, 'could': 14, 'be': 5, 'some': 46, 'kind': 31, 'of': 36, 'trick': 56, 'budd': 8, 'had': 21, 'thought': 52, 'up': 57, 'calling': 11, 'you': 62, 'mr': 33, 'nelson': 34, 'at': 4, 'the': 50, 'request': 42, 'phillip': 39, 'wycoff': 61, 'senate': 44, 'quickly': 40, 'whipped': 60, 'through': 54, 'its': 30, 'meager': 32, 'fare': 18, 'house': 27, 'bills': 6, 'approved': 3, 'by': 9, 'committees': 12, 'passing': 38, 'three': 53, 'on': 37, 'calendar': 10, 'he': 23, 'expected': 17, 'concetta': 13, 'thin': 51, 'hand': 22, 'to': 55, 'reach': 41, 'down': 16, 'grasp': 20, 'boy': 7, 'and': 1, 'her': 25, 'shrill': 45, 'impetuous': 28, 'voice': 58, 'sound': 47, 'against': 0, 'rotundity': 43, 'his': 26, 'disfigured': 15, 'flesh': 19, 'that': 49, 'was': 59, 'never': 35, 'sure': 48, 'hearing': 24, 'anything': 2}\n"
     ]
    }
   ],
   "source": [
    "#mapping between unique words in dictonary and number assigned to them\n",
    "print(Cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab:  63\n",
      "Size of Vectorized sentence:  63\n"
     ]
    }
   ],
   "source": [
    "# They are same!\n",
    "print(\"Size of Vocab: \",len(Cv.vocabulary_.keys()))\n",
    "print(\"Size of Vectorized sentence: \",len(vectorized_corpus.toarray()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus=vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerica_form=vectorized_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerica_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['approved', 'bills', 'by', 'calendar', 'committees', 'fare',\n",
       "        'house', 'its', 'meager', 'of', 'on', 'passing', 'quickly',\n",
       "        'senate', 'the', 'three', 'through', 'whipped'], dtype='<U10')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate sentence out of numeric form\n",
    "# words are jumbled - bag of words\n",
    "Cv.inverse_transform(numerica_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The senate quickly whipped through its meager fare of House bills approved by committees,passing the three on the calendar.\n"
     ]
    }
   ],
   "source": [
    "print(sample_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization with Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! Mic testing.. 1, 2, 3 Great its working\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myToken(text):\n",
    "    words=tokenizer.tokenize(text.lower())\n",
    "    #Indian and indian considered same\n",
    "    \n",
    "    words=remove_sw(words,eng_sw)\n",
    "    #Remove Stopwords\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'mic', 'testing..', 'great', 'working']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myToken(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv1=CountVectorizer(tokenizer=myToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus1=Cv1.fit_transform(sample_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length Reduced from 63 to 42 cuz of stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['budd', 'could', 'kind', 'thought', 'trick', 'up.'], dtype='<U10'),\n",
       " array(['calling', 'mr.', 'nelson', 'phillip', 'request', 'wycoff.'],\n",
       "       dtype='<U10'),\n",
       " array(['approved', 'bills', 'calendar.', 'committees', 'fare', 'house',\n",
       "        'meager', 'passing', 'quickly', 'senate', 'three', 'whipped'],\n",
       "       dtype='<U10'),\n",
       " array(['anything.', 'boy', 'concetta', 'disfigured', 'expected', 'flesh',\n",
       "        'grasp', 'hand', 'hearing', 'impetuous', 'never', 'reach',\n",
       "        'rotundity', 'shrill', 'sound', 'sure', 'thin', 'voice'],\n",
       "       dtype='<U10')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cv1.inverse_transform(vectorized_corpus1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['be', 'budd', 'could', 'had', 'it', 'kind', 'of', 'some',\n",
       "        'thought', 'trick', 'up'], dtype='<U10'),\n",
       " array(['at', 'calling', 'mr', 'nelson', 'of', 'phillip', 'request', 'the',\n",
       "        'wycoff', 'you'], dtype='<U10'),\n",
       " array(['approved', 'bills', 'by', 'calendar', 'committees', 'fare',\n",
       "        'house', 'its', 'meager', 'of', 'on', 'passing', 'quickly',\n",
       "        'senate', 'the', 'three', 'through', 'whipped'], dtype='<U10'),\n",
       " array(['against', 'and', 'anything', 'boy', 'concetta', 'disfigured',\n",
       "        'down', 'expected', 'flesh', 'grasp', 'hand', 'he', 'hearing',\n",
       "        'her', 'his', 'impetuous', 'never', 'of', 'reach', 'rotundity',\n",
       "        'shrill', 'sound', 'sure', 'that', 'the', 'thin', 'to', 'voice',\n",
       "        'was'], dtype='<U10')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cv.inverse_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less words in Cv1 than original Cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Concepts\n",
    "'good movie' needs to be separated from 'not good movie'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features in multiple ways\n",
    "- Unigram: every word as a feature\n",
    "- Bigram: combine two words as a single feature\n",
    "- Trigram: combine three words as a single feature\n",
    "- ngrams: Combination of all three mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigram feature: inc size of vector but allow negation consideration\n",
    "Cv_bi=CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It could be some kind of trick Budd had thought up.',\n",
       " \"I'm calling you , Mr. Nelson , at the request of Mr. Phillip Wycoff.\",\n",
       " 'The senate quickly whipped through its meager fare of House bills approved by committees,passing the three on the calendar.',\n",
       " \"He expected Concetta's thin hand to reach down to grasp the boy,and her shrill,impetuous voice to sound against the rotundity of his disfigured flesh that was never sure of hearing anything.\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cv_bi.fit_transform(sample_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it could': 27, 'could be': 12, 'be some': 4, 'some kind': 49, 'kind of': 29, 'of trick': 39, 'trick budd': 66, 'budd had': 7, 'had thought': 19, 'thought up': 60, 'calling you': 9, 'you mr': 70, 'mr nelson': 31, 'nelson at': 33, 'at the': 3, 'the request': 55, 'request of': 45, 'of mr': 38, 'mr phillip': 32, 'phillip wycoff': 42, 'the senate': 57, 'senate quickly': 47, 'quickly whipped': 43, 'whipped through': 69, 'through its': 62, 'its meager': 28, 'meager fare': 30, 'fare of': 16, 'of house': 37, 'house bills': 25, 'bills approved': 5, 'approved by': 2, 'by committees': 8, 'committees passing': 10, 'passing the': 41, 'the three': 58, 'three on': 61, 'on the': 40, 'the calendar': 54, 'he expected': 21, 'expected concetta': 15, 'concetta thin': 11, 'thin hand': 59, 'hand to': 20, 'to reach': 64, 'reach down': 44, 'down to': 14, 'to grasp': 63, 'grasp the': 18, 'the boy': 53, 'boy and': 6, 'and her': 1, 'her shrill': 23, 'shrill impetuous': 48, 'impetuous voice': 26, 'voice to': 67, 'to sound': 65, 'sound against': 50, 'against the': 0, 'the rotundity': 56, 'rotundity of': 46, 'of his': 36, 'his disfigured': 24, 'disfigured flesh': 13, 'flesh that': 17, 'that was': 52, 'was never': 68, 'never sure': 34, 'sure of': 51, 'of hearing': 35, 'hearing anything': 22}\n"
     ]
    }
   ],
   "source": [
    "#Two features are combined\n",
    "print(Cv_bi.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigram feature: inc size of vector but allow negation consideration\n",
    "Cv_tri=CountVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cv_tri.fit_transform(sample_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it could be': 26, 'could be some': 12, 'be some kind': 4, 'some kind of': 47, 'kind of trick': 28, 'of trick budd': 38, 'trick budd had': 62, 'budd had thought': 7, 'had thought up': 19, 'calling you mr': 9, 'you mr nelson': 66, 'mr nelson at': 30, 'nelson at the': 32, 'at the request': 3, 'the request of': 52, 'request of mr': 43, 'of mr phillip': 37, 'mr phillip wycoff': 31, 'the senate quickly': 54, 'senate quickly whipped': 45, 'quickly whipped through': 41, 'whipped through its': 65, 'through its meager': 58, 'its meager fare': 27, 'meager fare of': 29, 'fare of house': 16, 'of house bills': 36, 'house bills approved': 24, 'bills approved by': 5, 'approved by committees': 2, 'by committees passing': 8, 'committees passing the': 10, 'passing the three': 40, 'the three on': 55, 'three on the': 57, 'on the calendar': 39, 'he expected concetta': 21, 'expected concetta thin': 15, 'concetta thin hand': 11, 'thin hand to': 56, 'hand to reach': 20, 'to reach down': 60, 'reach down to': 42, 'down to grasp': 14, 'to grasp the': 59, 'grasp the boy': 18, 'the boy and': 51, 'boy and her': 6, 'and her shrill': 1, 'her shrill impetuous': 22, 'shrill impetuous voice': 46, 'impetuous voice to': 25, 'voice to sound': 63, 'to sound against': 61, 'sound against the': 48, 'against the rotundity': 0, 'the rotundity of': 53, 'rotundity of his': 44, 'of his disfigured': 35, 'his disfigured flesh': 23, 'disfigured flesh that': 13, 'flesh that was': 17, 'that was never': 50, 'was never sure': 64, 'never sure of': 33, 'sure of hearing': 49, 'of hearing anything': 34}\n"
     ]
    }
   ],
   "source": [
    "#Three features are combined\n",
    "print(Cv_tri.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram feature\n",
    "Cv_ng=CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        3, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cv_ng.fit_transform(sample_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 82, 'could': 38, 'be': 13, 'some': 142, 'kind': 88, 'of': 105, 'trick': 184, 'budd': 22, 'had': 59, 'thought': 169, 'up': 187, 'it could': 83, 'could be': 39, 'be some': 14, 'some kind': 143, 'kind of': 89, 'of trick': 114, 'trick budd': 185, 'budd had': 23, 'had thought': 60, 'thought up': 170, 'it could be': 84, 'could be some': 40, 'be some kind': 15, 'some kind of': 144, 'kind of trick': 90, 'of trick budd': 115, 'trick budd had': 186, 'budd had thought': 24, 'had thought up': 61, 'calling': 29, 'you': 198, 'mr': 94, 'nelson': 99, 'at': 10, 'the': 154, 'request': 130, 'phillip': 122, 'wycoff': 197, 'calling you': 30, 'you mr': 199, 'mr nelson': 95, 'nelson at': 100, 'at the': 11, 'the request': 158, 'request of': 131, 'of mr': 112, 'mr phillip': 97, 'phillip wycoff': 123, 'calling you mr': 31, 'you mr nelson': 200, 'mr nelson at': 96, 'nelson at the': 101, 'at the request': 12, 'the request of': 159, 'request of mr': 132, 'of mr phillip': 113, 'mr phillip wycoff': 98, 'senate': 136, 'quickly': 124, 'whipped': 194, 'through': 174, 'its': 85, 'meager': 91, 'fare': 50, 'house': 76, 'bills': 16, 'approved': 7, 'by': 25, 'committees': 32, 'passing': 119, 'three': 171, 'on': 116, 'calendar': 28, 'the senate': 162, 'senate quickly': 137, 'quickly whipped': 125, 'whipped through': 195, 'through its': 175, 'its meager': 86, 'meager fare': 92, 'fare of': 51, 'of house': 110, 'house bills': 77, 'bills approved': 17, 'approved by': 8, 'by committees': 26, 'committees passing': 33, 'passing the': 120, 'the three': 164, 'three on': 172, 'on the': 117, 'the calendar': 157, 'the senate quickly': 163, 'senate quickly whipped': 138, 'quickly whipped through': 126, 'whipped through its': 196, 'through its meager': 176, 'its meager fare': 87, 'meager fare of': 93, 'fare of house': 52, 'of house bills': 111, 'house bills approved': 78, 'bills approved by': 18, 'approved by committees': 9, 'by committees passing': 27, 'committees passing the': 34, 'passing the three': 121, 'the three on': 165, 'three on the': 173, 'on the calendar': 118, 'he': 65, 'expected': 47, 'concetta': 35, 'thin': 166, 'hand': 62, 'to': 177, 'reach': 127, 'down': 44, 'grasp': 56, 'boy': 19, 'and': 3, 'her': 70, 'shrill': 139, 'impetuous': 79, 'voice': 188, 'sound': 145, 'against': 0, 'rotundity': 133, 'his': 73, 'disfigured': 41, 'flesh': 53, 'that': 151, 'was': 191, 'never': 102, 'sure': 148, 'hearing': 68, 'anything': 6, 'he expected': 66, 'expected concetta': 48, 'concetta thin': 36, 'thin hand': 167, 'hand to': 63, 'to reach': 180, 'reach down': 128, 'down to': 45, 'to grasp': 178, 'grasp the': 57, 'the boy': 155, 'boy and': 20, 'and her': 4, 'her shrill': 71, 'shrill impetuous': 140, 'impetuous voice': 80, 'voice to': 189, 'to sound': 182, 'sound against': 146, 'against the': 1, 'the rotundity': 160, 'rotundity of': 134, 'of his': 108, 'his disfigured': 74, 'disfigured flesh': 42, 'flesh that': 54, 'that was': 152, 'was never': 192, 'never sure': 103, 'sure of': 149, 'of hearing': 106, 'hearing anything': 69, 'he expected concetta': 67, 'expected concetta thin': 49, 'concetta thin hand': 37, 'thin hand to': 168, 'hand to reach': 64, 'to reach down': 181, 'reach down to': 129, 'down to grasp': 46, 'to grasp the': 179, 'grasp the boy': 58, 'the boy and': 156, 'boy and her': 21, 'and her shrill': 5, 'her shrill impetuous': 72, 'shrill impetuous voice': 141, 'impetuous voice to': 81, 'voice to sound': 190, 'to sound against': 183, 'sound against the': 147, 'against the rotundity': 2, 'the rotundity of': 161, 'rotundity of his': 135, 'of his disfigured': 109, 'his disfigured flesh': 75, 'disfigured flesh that': 43, 'flesh that was': 55, 'that was never': 153, 'was never sure': 193, 'never sure of': 104, 'sure of hearing': 150, 'of hearing anything': 107}\n"
     ]
    }
   ],
   "source": [
    "# All three types present\n",
    "print(Cv_ng.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These ways require more memory as vocab is bigger and hence vector length also increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-iDF Normalization\n",
    "TF: Term Frequency and,\n",
    "\n",
    "iDF: inverse Document Frequency\n",
    "\n",
    "Avoid features that occur very often - contain less info (info dec as no of occ inc)\n",
    "\n",
    "Example:\n",
    "\n",
    "'the' can occur a lot irrespective of the corpus category but \n",
    "On the other hand...\n",
    "\n",
    "'cricket' can occur a lot in cricket category but will occur very less in other categories.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    'this is a good movie',\n",
    "    'this was a good movie',\n",
    "    'this is not a good movie'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vc=tf_idf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3645444 , 0.46941728, 0.3645444 , 0.61722732, 0.3645444 ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'not' has index as 3 so the value at 3rd index is highest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
