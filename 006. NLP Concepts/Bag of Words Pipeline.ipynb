{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Pipeline\n",
    "1. Get the Data/Corpus\n",
    "2. Tokenisation,Stopword Removal\n",
    "3. Stemming\n",
    "4. Building a Vocab\n",
    "5. Vectorization\n",
    "6. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Hello there! My name is Isha. Today is a very pleasant day and the weather is cool. :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting separate sentences\n",
    "t1=sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there!', 'My name is Isha.', 'Today is a very pleasant day and the weather is cool.', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting separate words/special characters\n",
    "t2=word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', '!', 'My', 'name', 'is', 'Isha', '.', 'Today', 'is', 'a', 'very', 'pleasant', 'day', 'and', 'the', 'weather', 'is', 'cool', '.', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It contains stopwords from all the languages but we only consider english stopwords\n",
    "eng_sw=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all', 'when', \"mustn't\", 's', \"don't\", 'its', 'they', 'my', 'these', 'his', 'from', 'did', \"couldn't\", 'so', 'as', 'such', 'those', 'no', 'while', 'why', 'it', 've', 'than', \"hadn't\", 'yourselves', 'herself', 'some', 'and', 'against', 'does', 'further', 'in', \"aren't\", \"you'd\", 'by', 'nor', 'not', \"won't\", \"you're\", 'himself', 'whom', 'has', 'themselves', 'couldn', 'wasn', 'had', 'up', 'were', 'doesn', 'having', \"mightn't\", 'most', \"shouldn't\", \"wouldn't\", 'she', 'because', 'now', \"that'll\", \"hasn't\", 're', 'there', 'out', 'both', 'which', 'below', 'other', 'here', 'm', 'the', 'theirs', 'over', 'at', 'each', 'on', 'again', 'once', 'hasn', 'him', 'off', 'be', 'mightn', 'is', 'myself', 'a', 'll', 'any', 'd', 'wouldn', 'was', 'between', \"it's\", 'if', 'do', 'about', 'that', 'an', 'are', 'your', \"you've\", 'above', 'more', 'can', \"you'll\", 'few', 'i', \"doesn't\", 'our', 'itself', 'their', 'being', 'don', 'isn', \"haven't\", 'them', 'ours', \"she's\", 'after', 't', 'or', 'you', 'shouldn', 'didn', 'ourselves', 'will', 'he', 'how', 'too', 'o', 'through', \"wasn't\", 'down', 'until', 'but', \"weren't\", 'who', 'won', 'mustn', 'ma', 'where', 'shan', 'for', 'me', 'her', 'should', \"didn't\", 'same', 'we', 'just', \"isn't\", 'needn', \"needn't\", 'ain', 'hadn', \"shan't\", 'own', 'have', 'with', 'into', 'y', 'doing', 'to', 'very', 'am', 'haven', 'during', 'hers', 'been', 'only', 'aren', 'weren', 'under', 'yourself', 'then', 'of', 'this', 'what', 'yours', 'before', \"should've\"}\n"
     ]
    }
   ],
   "source": [
    "print(eng_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(text,sw):\n",
    "    useful_words=[i for i in text if i not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=remove_sw('I am happy with my life'.split(),eng_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'happy', 'life']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=remove_sw('Hello there! My name is Isha. Today is a very pleasant day and the weather is cool.'.split(),eng_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there!', 'My', 'name', 'Isha.', 'Today', 'pleasant', 'day', 'weather', 'cool.']\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation using Regular Expression\n",
    "https://www.regexpal.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Hello there! Mic testing.. 1, 2, 3 Great its working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text=tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'there', 'Mic', 'testing..', 'Great', 'its', 'working']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Three types of Stemmer:\n",
    "    \n",
    "    1. Snowball\n",
    "    2.Porter\n",
    "    3.Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of a stemmer\n",
    "Ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ps.stem('sitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-language Stemmer - specify lamguage\n",
    "Ss=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ss.stem('sitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wn=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wn.lemmatize('watching')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Vocab and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adventure,mystry,news,romance\n",
    "sample_corpus=[\n",
    "    \"It could be some kind of trick Budd had thought up.\",\n",
    "    \"I'm calling you , Mr. Nelson , at the request of Mr. Phillip Wycoff.\",\n",
    "    \"The senate quickly whipped through its meager fare of House bills approved by committees,passing the three on the calendar.\",\n",
    "    \"He expected Concetta's thin hand to reach down to grasp the boy,and her shrill,impetuous voice to sound against the rotundity of his disfigured flesh that was never sure of hearing anything.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer: \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus=Cv.fit_transform(sample_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 29)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 46)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 56)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 57)\t1\n",
      "  (1, 36)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 62)\t1\n",
      "  (1, 33)\t2\n",
      "  (1, 34)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 50)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 61)\t1\n",
      "  (2, 36)\t1\n",
      "  (2, 50)\t3\n",
      "  (2, 44)\t1\n",
      "  (2, 40)\t1\n",
      "  :\t:\n",
      "  (3, 13)\t1\n",
      "  (3, 51)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 55)\t3\n",
      "  (3, 41)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 20)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 25)\t1\n",
      "  (3, 45)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 58)\t1\n",
      "  (3, 47)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 43)\t1\n",
      "  (3, 26)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 19)\t1\n",
      "  (3, 49)\t1\n",
      "  (3, 59)\t1\n",
      "  (3, 35)\t1\n",
      "  (3, 48)\t1\n",
      "  (3, 24)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  It could be some kind of trick Budd had thought up.\n",
      "Vectorized form:  [0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#first line\n",
    "print(\"Original: \",sample_corpus[0])\n",
    "print(\"Vectorized form: \",vectorized_corpus.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 29, 'could': 14, 'be': 5, 'some': 46, 'kind': 31, 'of': 36, 'trick': 56, 'budd': 8, 'had': 21, 'thought': 52, 'up': 57, 'calling': 11, 'you': 62, 'mr': 33, 'nelson': 34, 'at': 4, 'the': 50, 'request': 42, 'phillip': 39, 'wycoff': 61, 'senate': 44, 'quickly': 40, 'whipped': 60, 'through': 54, 'its': 30, 'meager': 32, 'fare': 18, 'house': 27, 'bills': 6, 'approved': 3, 'by': 9, 'committees': 12, 'passing': 38, 'three': 53, 'on': 37, 'calendar': 10, 'he': 23, 'expected': 17, 'concetta': 13, 'thin': 51, 'hand': 22, 'to': 55, 'reach': 41, 'down': 16, 'grasp': 20, 'boy': 7, 'and': 1, 'her': 25, 'shrill': 45, 'impetuous': 28, 'voice': 58, 'sound': 47, 'against': 0, 'rotundity': 43, 'his': 26, 'disfigured': 15, 'flesh': 19, 'that': 49, 'was': 59, 'never': 35, 'sure': 48, 'hearing': 24, 'anything': 2}\n"
     ]
    }
   ],
   "source": [
    "#mapping between unique words in dictonary and number assigned to them\n",
    "print(Cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab:  63\n",
      "Size of Vectorized sentence:  63\n"
     ]
    }
   ],
   "source": [
    "# They are same!\n",
    "print(\"Size of Vocab: \",len(Cv.vocabulary_.keys()))\n",
    "print(\"Size of Vectorized sentence: \",len(vectorized_corpus.toarray()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus=vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerica_form=vectorized_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerica_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['approved', 'bills', 'by', 'calendar', 'committees', 'fare',\n",
       "        'house', 'its', 'meager', 'of', 'on', 'passing', 'quickly',\n",
       "        'senate', 'the', 'three', 'through', 'whipped'], dtype='<U10')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate sentence out of numeric form\n",
    "# words are jumbled - bag of words\n",
    "Cv.inverse_transform(numerica_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The senate quickly whipped through its meager fare of House bills approved by committees,passing the three on the calendar.\n"
     ]
    }
   ],
   "source": [
    "print(sample_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization with Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! Mic testing.. 1, 2, 3 Great its working\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myToken(text):\n",
    "    words=tokenizer.tokenize(text.lower())\n",
    "    #Indian and indian considered same\n",
    "    \n",
    "    words=remove_sw(words,eng_sw)\n",
    "    #Remove Stopwords\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'mic', 'testing..', 'great', 'working']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myToken(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv1=CountVectorizer(tokenizer=myToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus1=Cv1.fit_transform(sample_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length Reduced from 63 to 42 cuz of stopwords Removal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
